{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Needed? Detecting Sentences from Computing Research Papers that Need Citations\n",
    "Author: Nicholas Vincent, Northwestern University\n",
    "\n",
    "email: nickvincent@u.northwestern.edu | lab website: [www.psagroup.org](http://www.psagroup.org) | personal website: [www.nickmvincent.com](http://www.nickmvincent.com)\n",
    "\n",
    "Prof. Doug Downey's EECS349 course, final project\n",
    "\n",
    "## Abstract: \n",
    "The scientific paper is the primary artifact produced by scientists of every discipline. Within a scientific paper, citations are incredibly valuable: they help connect a paper to the surrounding literature, provide evidence for claims, and empower a single 10-page PDF to “stand on the shoulders of giants” by referencing prior work. However, language is ambiguous, and it may not always be trivial to decide whether a certain sentence should include a citation or not.  Therefore, it may be valuable for an author or reviewer to be able to quickly identify whether a sentence should include a citation or not. We implement and test a variety of machine learning classifiers that attempt to solve this task.\n",
    "\n",
    "In this work, we explore two distinct approaches to classification: (1) using word-level features (i.e. bag-of-words) alongside textual metadata with traditional classification techniques (Naive Bayes, trees, logistic regression, etc) and (2) using character-level features with a deep learning techniques (recurrent neural networks, long-short term memory). We find that for a small dataset (every sentence from 20 computing research papers) logistic regression performs the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataset\n",
    "We generated all training and test data by downloading computing papers as PDF and converting them to text (detailed below). Each text file was processed so as to split the text into sentences (using NLTK sentence tokenizer with some domain-specific tweaks), identify all the sentences with citations, and then strip out the evidence of the citations (in the case papers we used, this was bracketed numbers, like [1]) so the data could be fairly for training.\n",
    "\n",
    "We built two datasets for developing and testing: first we used a \"small\" dataset composed of X papers. This dataset included X different sentences and 11% (X) had citations.\n",
    "\n",
    "\n",
    "## Overview of Features and Classification Approaches Used\n",
    "\n",
    "## Results\n",
    "We found that...\n",
    "\n",
    "## Future Work\n",
    "Although...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
